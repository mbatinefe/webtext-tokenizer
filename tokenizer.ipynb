{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TXP5nalr9ls"
      },
      "source": [
        "# **My Tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "-jYyH_qz3Lii"
      },
      "outputs": [],
      "source": [
        "def my_tokenizer(corpus_raw):\n",
        "    '''\n",
        "    type corpus_raw: string\n",
        "    param corpus_raw: The raw output of the corpus to be tokenized\n",
        "    rtype: list\n",
        "    return: a list of tokens extracted from the corpus_raw\n",
        "    '''\n",
        "\n",
        "    # Convert raw text to lowercase\n",
        "    corpus_raw = corpus_raw.lower()\n",
        "    # Tokenization with Regular Expression (explained in detail on report)\n",
        "    pattern = r\"\\w+(?:[-']\\w+)*|[.,!?;:]\"\n",
        "    # Find this pattern on raw corpus and get a list\n",
        "    token_list = re.findall(pattern, corpus_raw)\n",
        "\n",
        "    clean_token_list = [] # Temporary list to hold cleaned tokens\n",
        "    for token in token_list:\n",
        "        token = token.strip() # Remove whitespaces\n",
        "        if token == \"\":\n",
        "          # Skip if it is empty\n",
        "          continue\n",
        "        if len(token) == 1 and not token.isalnum():\n",
        "          # Skip if token is single and non alphanumeric\n",
        "          continue\n",
        "\n",
        "        # Add cleaned token to new list\n",
        "        clean_token_list.append(token)\n",
        "\n",
        "    token_list = clean_token_list\n",
        "\n",
        "    return token_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZF4WJjKxZfc",
        "outputId": "d2661466-09e8-409b-eb83-22a5fc210aa7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]   Package webtext is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import re\n",
        "\n",
        "corpus_name = 'webtext'\n",
        "\n",
        "#download the corpus and import it.\n",
        "nltk.download(corpus_name)\n",
        "from nltk.corpus import webtext\n",
        "\n",
        "#get the raw text output of the corpus to the corpus_raw variable.\n",
        "corpus_raw = webtext.raw()\n",
        "\n",
        "#call your tokenizer method\n",
        "my_tokenized_list = my_tokenizer(corpus_raw)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuTDStwY36cT"
      },
      "source": [
        "## Evaluate the tokenizer with the nltk word tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "8txzw8Ag8ysD"
      },
      "outputs": [],
      "source": [
        "def similarity_score(set_a, set_b):\n",
        "    '''\n",
        "    type set_a: set\n",
        "    param set_a: The first set to be compared\n",
        "    type set_b: set\n",
        "    param set_b: The tokens extracted from the corpus_raw\n",
        "    rtype: float\n",
        "    return: similarity score with two sets using Jaccard similarity.\n",
        "    '''\n",
        "\n",
        "    jaccard_similarity = float(len(set_a.intersection(set_b)) / len(set_a.union(set_b)))\n",
        "\n",
        "    return jaccard_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wUTqReb36Hg",
        "outputId": "dce5517f-021e-4f13-c2c0-9e14c9d4ed47"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from nltk import word_tokenize\n",
        "nltk.download('punkt')\n",
        "from nltk import punkt\n",
        "\n",
        "def evaluation(corpus_raw, token_list):\n",
        "    '''\n",
        "    type corpus_raw: string\n",
        "    param corpus_raw: The raw output of the corpus\n",
        "    type token_list: list\n",
        "    param token_list: The tokens extracted from the corpus_raw\n",
        "    rtype: float\n",
        "    return: comparison score with the given token list and the nltk tokenizer.\n",
        "    '''\n",
        "\n",
        "    #The comparison score only looks at the tokens but not the frequencies of the tokens.\n",
        "    #we assume case folding is already applied to the token_list\n",
        "    corpus_raw = corpus_raw.lower()\n",
        "    nltk_tokens = word_tokenize(corpus_raw, language='english')\n",
        "\n",
        "    score = similarity_score(set(token_list), set(nltk_tokens))\n",
        "\n",
        "    return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Vt_uSBF-NHm",
        "outputId": "d48c52e5-6add-4244-d98e-139f8c1c6c06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The similarity score is 0.86\n"
          ]
        }
      ],
      "source": [
        "#Evaluation\n",
        "\n",
        "eval_score = evaluation(corpus_raw, my_tokenized_list)\n",
        "\n",
        "print('The similarity score is {:.2f}'.format(eval_score))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
